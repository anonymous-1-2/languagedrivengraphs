Language-Driven Graphs for Short Video Similarity
Project Banner

Final comparison of the semantic cohesion of similarity graphs built from human descriptions, AI-generated (LLM) descriptions, and pure visual features (CLIP).

Project Overview
This repository contains the code, processed data, and research results comparing the effectiveness of different modalities in building similarity graphs for short videos. Using the MSR-VTT dataset, this project constructs and evaluates three types of graphs:

Visual Graph: Based on the cosine similarity between visual embeddings extracted with CLIP.
Human Description Graph: Based on the semantic similarity (SBERT) between human-provided descriptions from the dataset.
AI-Generated Description Graph: Based on the semantic similarity (SBERT) between descriptions generated by an LLM, specifically smol-vlm-2.0.
Workflow and Execution
The process is divided into two Jupyter Notebooks, designed to be run sequentially in Google Colab.

Notebook 1: notebooks/01_Data_Preprocessing_and_Feature_Extraction.ipynb (Optional)
    This notebook performs the complete preprocessing pipeline, starting from the raw MSR-VTT data. You only need to run this if you want to replicate the feature extraction from scratch.

Notebook 2: notebooks/02_Graph_Analysis_and_Evaluation.ipynb (Main)
    This notebook conducts the core analysis. It loads the pre-processed data and features, builds the graphs, evaluates their semantic cohesion, and generates the final results plot.

How to Reproduce the Results
Step 1: Google Drive Environment Setup
Clone the Repository to Your Local Machine:

bash
git clone https://github.com/anonymous-1-2/languagedrivengraphs.git
Create the Project Folder in Your Google Drive:
In the root of your Google Drive (My Drive), create the following folder structure:

markdown
My Drive/
└── D/
    └── Dataset/
Upload the Project Files:
Upload all .ipynb, .csv, .xlsx files and the figures/ folder that you cloned from GitHub into the My Drive/D/Dataset/ directory you just created.

Add Shortcuts to Data and Features:
Open the shared Google Drive links below. For each link, instead of downloading, click "Add shortcut to Drive" (or the triangle icon with a +) and select the My Drive/D/Dataset/ folder as the destination.

Required Shortcuts for Analysis (Notebook 2 - Recommended):

Keyframes/ (Link)
MSRVTT_Features_ViT_L_14_Aggregated/ (Link)
MSRVTT_Features_ViT_L_14_Sequential/ (Link)
Shortcuts for Full Feature Generation (Optional):
If you wish to run Notebook 1 from scratch, also add shortcuts for the raw data:

TrainValVideo.zip (Link)
train_val_videodatainfo.json
Step 2: Running the Notebooks
Option A: Run Analysis Only (Recommended)

After setting up your Drive environment with the shortcuts, open the notebooks/02_Graph_Analysis_and_Evaluation.ipynb notebook in Google Colab.
Important Action: In the Google Colab file browser (left-side panel), click the "Upload to session storage" icon and select the MSRVTT_dados_compilados_com_features.xlsx file from your local machine. This will make it directly accessible at the /content/ path.
Run all cells in the notebook to perform the complete analysis and generate the final results.
Option B: Full Generation from Scratch

Ensure the shortcuts for the raw data (TrainValVideo.zip and the .json file) have been added to your Drive.
Open the notebooks/01_Data_Preprocessing_and_Feature_Extraction.ipynb notebook in Google Colab with a GPU runtime (T4 or higher).
Run all cells. This process is time-consuming and may take several hours. The outputs, including the MSRVTT_dados_compilados_com_features.xlsx file, will be saved to your Google Drive.
Once complete, proceed with Option A, uploading the newly generated .xlsx file to the Colab session as instructed.
Dependencies
The main libraries are installed directly within the notebooks using !pip install commands. Key dependencies include: transformers, torch, sentence-transformers, networkx, pandas, scikit-learn, matplotlib, seaborn, and tqdm.

Author
GitHub: https://github.com/anonymous-1-2